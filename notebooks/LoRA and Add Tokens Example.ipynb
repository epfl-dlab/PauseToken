{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List\n",
    "import transformers\n",
    "from tokenizers import AddedToken\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWrapper(nn.Module):\n",
    "    def __init__(self, layer: nn.Linear, num_embeddings: int, freeze_old=True):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.n_new_tokens = num_embeddings - layer.out_features\n",
    "        self.new_embeddings = nn.Linear(layer.in_features, self.n_new_tokens, bias=False)\n",
    "        self.new_embeddings.to(layer.weight.device).to(layer.weight.dtype)\n",
    "        if freeze_old:\n",
    "            for param in self.layer.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.layer(x)\n",
    "        z2 = self.new_embeddings(x)\n",
    "        return torch.cat([z1, z2], dim=-1)\n",
    "\n",
    "class EmbeddingWrapper(nn.Module):\n",
    "    def __init__(self, embedding: nn.Embedding, num_embeddings: int, freeze_old=True):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding.embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.n_new_tokens = num_embeddings - embedding.num_embeddings\n",
    "        # inspired from here \n",
    "        # https://github.com/huggingface/transformers/blob/185463784e0a0b4cd7974ce5bded7a52ae170f6d/src/transformers/modeling_utils.py#L2026\n",
    "        self.old_embeddings = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.old_embeddings.weight.data = torch.ones_like(self.old_embeddings.weight.data)*0#1e-7\n",
    "        self.old_embeddings.weight.data[:embedding.num_embeddings] = embedding.weight.data\n",
    "        self.old_embeddings.to(embedding.weight.device).to(embedding.weight.dtype)\n",
    "        self.new_embeddings = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.new_embeddings.weight.data[:embedding.num_embeddings] = torch.ones_like(embedding.weight.data)*0#1e-7\n",
    "        self.new_embeddings.to(embedding.weight.device).to(embedding.weight.dtype)\n",
    "        if freeze_old:\n",
    "            for param in self.old_embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.old_embeddings(x) + self.new_embeddings(x)\n",
    "\n",
    "\n",
    "class Llama2EmbeddingSurgeon():\n",
    "    def __init__(self, llama, extended_tokenizer):\n",
    "        self.llama = llama \n",
    "        self.extended_tokenizer = extended_tokenizer\n",
    "        self.extended_embedding = EmbeddingWrapper(llama.model.embed_tokens, len(extended_tokenizer))\n",
    "        self.extended_unembedding = LinearWrapper(llama.lm_head, len(extended_tokenizer))\n",
    "        \n",
    "    def get_surgeried_model(self):\n",
    "        self.backup_embed_tokens = self.llama.model.embed_tokens\n",
    "        self.backup_lm_head = self.llama.lm_head\n",
    "        self.llama.model.embed_tokens = self.extended_embedding\n",
    "        self.llama.lm_head = self.extended_unembedding\n",
    "        self.llama.config.vocab_size = len(self.extended_tokenizer)\n",
    "        return self.llama\n",
    "    \n",
    "    def save(self, llama, path):\n",
    "        # check if llama is surgeried\n",
    "        assert llama.model.embed_tokens == self.extended_embedding\n",
    "        assert llama.lm_head == self.extended_unembedding\n",
    "        self.llama.model.embed_tokens = self.backup_embed_tokens\n",
    "        self.llama.lm_head = self.backup_lm_head\n",
    "        self.llama.save_pretrained(path)\n",
    "        self.extended_tokenizer.save_pretrained(path)\n",
    "        torch.save(self.extended_embedding.state_dict(), f\"{path}/extended_embedding.pt\")\n",
    "        torch.save(self.extended_unembedding.state_dict(), f\"{path}/extended_unembedding.pt\") \n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        extended_embedding_dict = torch.load(f\"{path}/extended_embedding.pt\")\n",
    "        extended_unembedding_dict = torch.load(f\"{path}/extended_unembedding.pt\")\n",
    "        llama = AutoModelForCausalLM.from_pretrained(path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        surgeon = cls(llama, tokenizer)\n",
    "        surgeon.extended_embedding.load_state_dict(extended_embedding_dict)\n",
    "        surgeon.extended_unembedding.load_state_dict(extended_unembedding_dict)\n",
    "        return surgeon\n",
    "\n",
    "class PeftModelEmbeddingSurgeon():\n",
    "    def __init__(self, peft_model, extended_tokenizer):\n",
    "        try:\n",
    "            self.llama = peft_model.base_model.model\n",
    "        except AttributeError:\n",
    "            self.llama = peft_model\n",
    "        self.peft_model = peft_model\n",
    "        self.extended_tokenizer = extended_tokenizer\n",
    "        self.extended_embedding = nn.Embedding(len(extended_tokenizer), self.llama.model.embed_tokens.embedding_dim)\n",
    "        self.extended_embedding.to(self.llama.model.embed_tokens.weight.device).to(self.llama.model.embed_tokens.weight.dtype)\n",
    "        self.extended_unembedding = nn.Linear(self.llama.model.embed_tokens.embedding_dim, len(extended_tokenizer), bias=False)\n",
    "        self.extended_unembedding.to(self.llama.lm_head.weight.device).to(self.llama.lm_head.weight.dtype)\n",
    "       # self.extended_embedding = EmbeddingWrapper(self.llama.model.embed_tokens, len(extended_tokenizer))\n",
    "       # self.extended_unembedding = LinearWrapper(self.llama.lm_head, len(extended_tokenizer))\n",
    "        \n",
    "    def get_surgeried_model(self):\n",
    "        self.backup_embed_tokens = self.llama.model.embed_tokens\n",
    "        self.backup_lm_head = self.llama.lm_head\n",
    "        self.llama.model.embed_tokens = self.extended_embedding\n",
    "        self.llama.lm_head = self.extended_unembedding\n",
    "        self.llama.config.vocab_size = len(self.extended_tokenizer)\n",
    "        return self.peft_model\n",
    "\n",
    "    def save(self, peft_model, path):        \n",
    "        self.llama.model.embed_tokens = self.backup_embed_tokens\n",
    "        self.llama.lm_head = self.backup_lm_head\n",
    "        self.peft_model.save_pretrained(path)\n",
    "        self.extended_tokenizer.save_pretrained(path)\n",
    "        torch.save(self.extended_embedding.state_dict(), f\"{path}/extended_embedding.pt\")\n",
    "        torch.save(self.extended_unembedding.state_dict(), f\"{path}/extended_unembedding.pt\") \n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path, **kwargs):\n",
    "        extended_embedding_dict = torch.load(f\"{path}/extended_embedding.pt\")\n",
    "        extended_unembedding_dict = torch.load(f\"{path}/extended_unembedding.pt\")\n",
    "        peft_model = AutoModelForCausalLM.from_pretrained(path, **kwargs)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        surgeon = cls(peft_model, tokenizer)\n",
    "        surgeon.extended_embedding.load_state_dict(extended_embedding_dict)\n",
    "        surgeon.extended_unembedding.load_state_dict(extended_unembedding_dict)\n",
    "        return surgeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378dee0c4dba47e19a052fd4f6dc0291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_or_path = '/dlabscratch1/public/llm_weights/llama2_hf/Llama-2-7b-hf/'\n",
    "# load model in torch.bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map='auto', torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 159,907,840 || all params: 6,898,323,456 || trainable%: 2.3180681656919973\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=64, \n",
    "    lora_alpha=16, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"fc1\", \"fc2\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",], \n",
    "    lora_dropout=0.00, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pause_token = AddedToken(\"<|pause|>\", \n",
    "                         single_word=False, \n",
    "                         lstrip=True, \n",
    "                         rstrip=True)\n",
    "                         #special=True, \n",
    "                         #normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='/dlabscratch1/public/llm_weights/llama2_hf/Llama-2-7b-hf/', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<|pause|>\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_tokens([pause_token], special_tokens=True)\n",
    "print(tokenizer)\n",
    "# get idx of pause otken\n",
    "pause_token_id = tokenizer.convert_tokens_to_ids(\"<|pause|>\")\n",
    "print(pause_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32001, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaSdpaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## conventionally you'd do this like this:\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "## ours\n",
    "surgeon = PeftModelEmbeddingSurgeon(model, tokenizer)\n",
    "model = surgeon.get_surgeried_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.Size([32001, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([64, 4096])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([11008, 64])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([64, 11008])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([4096, 64])\n",
      "base_model.model.lm_head.weight torch.Size([32001, 4096])\n"
     ]
    }
   ],
   "source": [
    "for k, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(k, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.0.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.1.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.2.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.3.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.4.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.5.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.6.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.7.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.8.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.9.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.10.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.11.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.12.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.13.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.14.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.base_layer.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight cuda:0\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight cuda:0\n",
      "base_model.model.model.layers.15.input_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight cuda:0\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.16.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.17.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.18.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.19.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.20.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.21.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.22.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.23.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.24.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.25.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.26.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.27.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.28.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.29.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.30.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.base_layer.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight cuda:1\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight cuda:1\n",
      "base_model.model.model.layers.31.input_layernorm.weight cuda:1\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight cuda:1\n",
      "base_model.model.model.norm.weight cuda:1\n",
      "base_model.model.lm_head.weight cuda:1\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w/o 1 <s> w 1 <s>\n",
      "w/o 450 The w 450 The\n",
      "skipping pause token...\n",
      "w/o 4996 quick w 4996 quick\n",
      "skipping pause token...\n",
      "w/o 17354 brown w 17354 brown\n",
      "skipping pause token...\n",
      "w/o 1701 fo w 1701 fo\n",
      "w/o 29916 x w 29916 x\n",
      "w/o 432 j w 432 j\n",
      "w/o 17204 umps w 17204 umps\n",
      "w/o 975 over w 975 over\n",
      "w/o 278 the w 278 the\n",
      "w/o 17366 lazy w 17366 lazy\n",
      "w/o 11203 dog w 11203 dog\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "toks1 = tokenizer.encode('The<|pause|> quick<|pause|> brown <|pause|> fox jumps over the lazy dog', return_tensors='pt')\n",
    "toks2 = tokenizer.encode('The quick brown fox jumps over the lazy dog', return_tensors='pt')\n",
    "idx2 = 0\n",
    "for idx1 in range(len(toks1[0])):\n",
    "    if toks1[0, idx1].item() != pause_token_id:\n",
    "        print('w/o', toks2[0, idx2].item(), tokenizer.decode([toks2[0, idx2]]), 'w', toks1[0, idx1].item(), tokenizer.decode([toks1[0, idx1]]))\n",
    "        assert toks2[0, idx2] == toks1[0, idx1]\n",
    "        idx2 += 1\n",
    "    else:\n",
    "        print('skipping pause token...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenizer.encode('The<|pause|> quick<|pause|> brown <|pause|> fox jumps over the lazy dog', return_tensors='pt'), max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog Kknow Email Weltkrie Щenschapp CreateDelcurrмериIsGEN Emailходить InterGEN cím pert có presenceestigoriousмери CreateParameters presenceIsromagnet K giant dispos javafxactor OUTbigg'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The<|pause|> quick<|pause|> brown<|pause|> fox jumps over the lazy dog Kknow Email Weltkrie Щenschapp CreateDelcurrмериIsGEN Emailходить InterGEN cím pert có presenceestigoriousмери CreateParameters presenceIsromagnet K giant dispos javafxactor OUTbigg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split = \"train\")\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = len(examples[\"question\"])*[\"Solve the math problem using a eval tool. The command eval[[expr]] allows you to evaluate an expression.\"]\n",
    "    inputs       = examples[\"question\"]\n",
    "    outputs      = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    #print(texts)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrisxx\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/dlabdata1/wendler/code/PauseToken/notebooks/wandb/run-20240411_210621-jm9mola1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/chrisxx/huggingface/runs/jm9mola1' target=\"_blank\">firm-dew-25</a></strong> to <a href='https://wandb.ai/chrisxx/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/chrisxx/huggingface' target=\"_blank\">https://wandb.ai/chrisxx/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/chrisxx/huggingface/runs/jm9mola1' target=\"_blank\">https://wandb.ai/chrisxx/huggingface/runs/jm9mola1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Attempting to unscale FP16 gradients.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/dlabdata1/wendler/code/PauseToken/notebooks/LoRA and Add Tokens Example.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m trainer \u001b[39m=\u001b[39m SFTTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     model \u001b[39m=\u001b[39m model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Biccluster039.iccluster.epfl.ch/dlabdata1/wendler/code/PauseToken/notebooks/LoRA%20and%20Add%20Tokens%20Example.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     trainer_stats \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:360\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    358\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trl_activate_neftune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m--> 360\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    362\u001b[0m \u001b[39m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/transformers/trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2159\u001b[0m     _grad_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2160\u001b[0m         amp\u001b[39m.\u001b[39mmaster_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer),\n\u001b[1;32m   2161\u001b[0m         args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2162\u001b[0m     )\n\u001b[1;32m   2163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2164\u001b[0m     _grad_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[1;32m   2165\u001b[0m         model\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m   2166\u001b[0m         args\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[1;32m   2167\u001b[0m     )\n\u001b[1;32m   2169\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2170\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2171\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m==\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2172\u001b[0m ):\n\u001b[1;32m   2173\u001b[0m     grad_norm \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/accelerate/accelerator.py:2157\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2155\u001b[0m             \u001b[39m# Set is_xla_gradients_synced to True to avoid all-reduce twice in the AcceleratedOptimizer step.\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m             acc_opt\u001b[39m.\u001b[39mgradient_state\u001b[39m.\u001b[39mis_xla_gradients_synced \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 2157\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munscale_gradients()\n\u001b[1;32m   2158\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[39m=\u001b[39mnorm_type)\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/accelerate/accelerator.py:2107\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2106\u001b[0m     opt \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39moptimizer\n\u001b[0;32m-> 2107\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49munscale_(opt)\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:336\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    333\u001b[0m inv_scale \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale\u001b[39m.\u001b[39mdouble()\u001b[39m.\u001b[39mreciprocal()\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m    334\u001b[0m found_inf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((), \u001b[39m0.0\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scale\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 336\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_unscale_grads_(\n\u001b[1;32m    337\u001b[0m     optimizer, inv_scale, found_inf, \u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    339\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mUNSCALED\n",
      "File \u001b[0;32m/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:258\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m allow_fp16) \u001b[39mand\u001b[39;00m param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16:\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to unscale FP16 gradients.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse:\n\u001b[1;32m    260\u001b[0m     \u001b[39m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[39m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m torch\u001b[39m.\u001b[39mfloat16:\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 1024,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        gradient_checkpointing=False,\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 0,\n",
    "        max_steps = 1,\n",
    "        #num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dlabdata1/wendler/.rlllm/lib/python3.11/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /dlabscratch1/public/llm_weights/llama2_hf/Llama-2-7b-hf/ - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "surgeon.save(model, '/dlabscratch1/tmp/peft_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=11008, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=11008, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "model.cpu()\n",
    "gc.collect()\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    torch.cuda.set_device(i) \n",
    "    torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d880299e2de48b0804bd43ce9607d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "surgeon2 = PeftModelEmbeddingSurgeon.load('/dlabscratch1/tmp/peft_test', device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = surgeon2.get_surgeried_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.old_embeddings.weight cpu\n",
      "model.embed_tokens.new_embeddings.weight cpu\n",
      "model.layers.0.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.0.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.0.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.0.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.0.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.0.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.0.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.0.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.0.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.0.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.0.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.0.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.0.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.0.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.0.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.0.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.0.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.0.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.0.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.0.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.0.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.0.input_layernorm.weight cpu\n",
      "model.layers.0.post_attention_layernorm.weight cpu\n",
      "model.layers.1.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.1.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.1.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.1.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.1.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.1.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.1.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.1.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.1.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.1.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.1.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.1.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.1.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.1.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.1.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.1.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.1.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.1.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.1.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.1.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.1.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.1.input_layernorm.weight cpu\n",
      "model.layers.1.post_attention_layernorm.weight cpu\n",
      "model.layers.2.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.2.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.2.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.2.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.2.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.2.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.2.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.2.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.2.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.2.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.2.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.2.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.2.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.2.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.2.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.2.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.2.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.2.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.2.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.2.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.2.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.2.input_layernorm.weight cpu\n",
      "model.layers.2.post_attention_layernorm.weight cpu\n",
      "model.layers.3.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.3.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.3.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.3.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.3.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.3.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.3.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.3.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.3.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.3.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.3.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.3.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.3.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.3.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.3.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.3.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.3.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.3.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.3.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.3.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.3.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.3.input_layernorm.weight cpu\n",
      "model.layers.3.post_attention_layernorm.weight cpu\n",
      "model.layers.4.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.4.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.4.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.4.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.4.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.4.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.4.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.4.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.4.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.4.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.4.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.4.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.4.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.4.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.4.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.4.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.4.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.4.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.4.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.4.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.4.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.4.input_layernorm.weight cpu\n",
      "model.layers.4.post_attention_layernorm.weight cpu\n",
      "model.layers.5.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.5.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.5.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.5.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.5.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.5.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.5.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.5.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.5.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.5.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.5.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.5.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.5.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.5.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.5.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.5.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.5.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.5.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.5.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.5.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.5.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.5.input_layernorm.weight cpu\n",
      "model.layers.5.post_attention_layernorm.weight cpu\n",
      "model.layers.6.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.6.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.6.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.6.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.6.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.6.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.6.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.6.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.6.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.6.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.6.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.6.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.6.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.6.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.6.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.6.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.6.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.6.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.6.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.6.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.6.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.6.input_layernorm.weight cpu\n",
      "model.layers.6.post_attention_layernorm.weight cpu\n",
      "model.layers.7.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.7.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.7.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.7.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.7.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.7.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.7.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.7.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.7.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.7.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.7.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.7.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.7.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.7.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.7.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.7.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.7.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.7.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.7.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.7.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.7.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.7.input_layernorm.weight cpu\n",
      "model.layers.7.post_attention_layernorm.weight cpu\n",
      "model.layers.8.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.8.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.8.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.8.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.8.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.8.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.8.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.8.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.8.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.8.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.8.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.8.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.8.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.8.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.8.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.8.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.8.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.8.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.8.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.8.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.8.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.8.input_layernorm.weight cpu\n",
      "model.layers.8.post_attention_layernorm.weight cpu\n",
      "model.layers.9.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.9.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.9.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.9.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.9.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.9.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.9.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.9.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.9.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.9.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.9.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.9.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.9.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.9.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.9.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.9.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.9.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.9.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.9.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.9.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.9.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.9.input_layernorm.weight cpu\n",
      "model.layers.9.post_attention_layernorm.weight cpu\n",
      "model.layers.10.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.10.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.10.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.10.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.10.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.10.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.10.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.10.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.10.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.10.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.10.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.10.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.10.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.10.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.10.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.10.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.10.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.10.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.10.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.10.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.10.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.10.input_layernorm.weight cpu\n",
      "model.layers.10.post_attention_layernorm.weight cpu\n",
      "model.layers.11.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.11.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.11.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.11.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.11.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.11.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.11.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.11.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.11.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.11.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.11.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.11.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.11.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.11.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.11.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.11.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.11.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.11.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.11.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.11.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.11.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.11.input_layernorm.weight cpu\n",
      "model.layers.11.post_attention_layernorm.weight cpu\n",
      "model.layers.12.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.12.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.12.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.12.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.12.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.12.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.12.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.12.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.12.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.12.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.12.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.12.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.12.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.12.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.12.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.12.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.12.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.12.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.12.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.12.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.12.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.12.input_layernorm.weight cpu\n",
      "model.layers.12.post_attention_layernorm.weight cpu\n",
      "model.layers.13.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.13.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.13.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.13.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.13.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.13.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.13.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.13.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.13.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.13.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.13.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.13.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.13.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.13.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.13.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.13.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.13.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.13.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.13.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.13.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.13.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.13.input_layernorm.weight cpu\n",
      "model.layers.13.post_attention_layernorm.weight cpu\n",
      "model.layers.14.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.14.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.14.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.14.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.14.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.14.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.14.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.14.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.14.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.14.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.14.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.14.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.14.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.14.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.14.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.14.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.14.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.14.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.14.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.14.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.14.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.14.input_layernorm.weight cpu\n",
      "model.layers.14.post_attention_layernorm.weight cpu\n",
      "model.layers.15.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.15.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.15.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.15.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.15.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.15.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.15.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.15.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.15.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.15.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.15.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.15.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.15.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.15.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.15.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.15.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.15.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.15.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.15.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.15.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.15.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.15.input_layernorm.weight cpu\n",
      "model.layers.15.post_attention_layernorm.weight cpu\n",
      "model.layers.16.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.16.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.16.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.16.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.16.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.16.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.16.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.16.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.16.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.16.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.16.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.16.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.16.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.16.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.16.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.16.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.16.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.16.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.16.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.16.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.16.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.16.input_layernorm.weight cpu\n",
      "model.layers.16.post_attention_layernorm.weight cpu\n",
      "model.layers.17.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.17.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.17.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.17.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.17.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.17.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.17.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.17.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.17.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.17.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.17.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.17.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.17.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.17.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.17.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.17.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.17.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.17.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.17.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.17.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.17.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.17.input_layernorm.weight cpu\n",
      "model.layers.17.post_attention_layernorm.weight cpu\n",
      "model.layers.18.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.18.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.18.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.18.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.18.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.18.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.18.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.18.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.18.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.18.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.18.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.18.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.18.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.18.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.18.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.18.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.18.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.18.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.18.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.18.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.18.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.18.input_layernorm.weight cpu\n",
      "model.layers.18.post_attention_layernorm.weight cpu\n",
      "model.layers.19.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.19.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.19.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.19.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.19.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.19.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.19.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.19.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.19.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.19.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.19.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.19.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.19.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.19.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.19.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.19.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.19.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.19.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.19.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.19.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.19.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.19.input_layernorm.weight cpu\n",
      "model.layers.19.post_attention_layernorm.weight cpu\n",
      "model.layers.20.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.20.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.20.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.20.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.20.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.20.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.20.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.20.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.20.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.20.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.20.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.20.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.20.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.20.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.20.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.20.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.20.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.20.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.20.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.20.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.20.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.20.input_layernorm.weight cpu\n",
      "model.layers.20.post_attention_layernorm.weight cpu\n",
      "model.layers.21.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.21.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.21.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.21.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.21.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.21.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.21.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.21.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.21.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.21.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.21.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.21.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.21.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.21.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.21.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.21.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.21.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.21.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.21.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.21.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.21.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.21.input_layernorm.weight cpu\n",
      "model.layers.21.post_attention_layernorm.weight cpu\n",
      "model.layers.22.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.22.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.22.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.22.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.22.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.22.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.22.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.22.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.22.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.22.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.22.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.22.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.22.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.22.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.22.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.22.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.22.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.22.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.22.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.22.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.22.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.22.input_layernorm.weight cpu\n",
      "model.layers.22.post_attention_layernorm.weight cpu\n",
      "model.layers.23.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.23.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.23.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.23.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.23.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.23.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.23.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.23.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.23.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.23.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.23.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.23.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.23.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.23.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.23.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.23.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.23.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.23.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.23.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.23.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.23.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.23.input_layernorm.weight cpu\n",
      "model.layers.23.post_attention_layernorm.weight cpu\n",
      "model.layers.24.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.24.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.24.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.24.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.24.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.24.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.24.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.24.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.24.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.24.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.24.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.24.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.24.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.24.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.24.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.24.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.24.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.24.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.24.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.24.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.24.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.24.input_layernorm.weight cpu\n",
      "model.layers.24.post_attention_layernorm.weight cpu\n",
      "model.layers.25.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.25.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.25.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.25.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.25.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.25.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.25.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.25.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.25.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.25.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.25.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.25.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.25.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.25.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.25.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.25.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.25.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.25.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.25.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.25.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.25.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.25.input_layernorm.weight cpu\n",
      "model.layers.25.post_attention_layernorm.weight cpu\n",
      "model.layers.26.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.26.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.26.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.26.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.26.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.26.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.26.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.26.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.26.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.26.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.26.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.26.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.26.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.26.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.26.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.26.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.26.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.26.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.26.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.26.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.26.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.26.input_layernorm.weight cpu\n",
      "model.layers.26.post_attention_layernorm.weight cpu\n",
      "model.layers.27.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.27.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.27.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.27.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.27.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.27.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.27.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.27.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.27.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.27.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.27.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.27.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.27.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.27.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.27.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.27.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.27.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.27.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.27.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.27.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.27.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.27.input_layernorm.weight cpu\n",
      "model.layers.27.post_attention_layernorm.weight cpu\n",
      "model.layers.28.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.28.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.28.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.28.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.28.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.28.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.28.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.28.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.28.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.28.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.28.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.28.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.28.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.28.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.28.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.28.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.28.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.28.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.28.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.28.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.28.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.28.input_layernorm.weight cpu\n",
      "model.layers.28.post_attention_layernorm.weight cpu\n",
      "model.layers.29.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.29.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.29.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.29.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.29.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.29.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.29.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.29.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.29.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.29.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.29.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.29.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.29.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.29.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.29.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.29.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.29.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.29.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.29.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.29.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.29.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.29.input_layernorm.weight cpu\n",
      "model.layers.29.post_attention_layernorm.weight cpu\n",
      "model.layers.30.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.30.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.30.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.30.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.30.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.30.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.30.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.30.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.30.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.30.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.30.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.30.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.30.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.30.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.30.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.30.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.30.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.30.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.30.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.30.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.30.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.30.input_layernorm.weight cpu\n",
      "model.layers.30.post_attention_layernorm.weight cpu\n",
      "model.layers.31.self_attn.q_proj.base_layer.weight cpu\n",
      "model.layers.31.self_attn.q_proj.lora_A.default.weight cpu\n",
      "model.layers.31.self_attn.q_proj.lora_B.default.weight cpu\n",
      "model.layers.31.self_attn.k_proj.base_layer.weight cpu\n",
      "model.layers.31.self_attn.k_proj.lora_A.default.weight cpu\n",
      "model.layers.31.self_attn.k_proj.lora_B.default.weight cpu\n",
      "model.layers.31.self_attn.v_proj.base_layer.weight cpu\n",
      "model.layers.31.self_attn.v_proj.lora_A.default.weight cpu\n",
      "model.layers.31.self_attn.v_proj.lora_B.default.weight cpu\n",
      "model.layers.31.self_attn.o_proj.base_layer.weight cpu\n",
      "model.layers.31.self_attn.o_proj.lora_A.default.weight cpu\n",
      "model.layers.31.self_attn.o_proj.lora_B.default.weight cpu\n",
      "model.layers.31.mlp.gate_proj.base_layer.weight cpu\n",
      "model.layers.31.mlp.gate_proj.lora_A.default.weight cpu\n",
      "model.layers.31.mlp.gate_proj.lora_B.default.weight cpu\n",
      "model.layers.31.mlp.up_proj.base_layer.weight cpu\n",
      "model.layers.31.mlp.up_proj.lora_A.default.weight cpu\n",
      "model.layers.31.mlp.up_proj.lora_B.default.weight cpu\n",
      "model.layers.31.mlp.down_proj.base_layer.weight cpu\n",
      "model.layers.31.mlp.down_proj.lora_A.default.weight cpu\n",
      "model.layers.31.mlp.down_proj.lora_B.default.weight cpu\n",
      "model.layers.31.input_layernorm.weight cpu\n",
      "model.layers.31.post_attention_layernorm.weight cpu\n",
      "model.norm.weight cpu\n",
      "lm_head.layer.weight cpu\n",
      "lm_head.new_embeddings.weight cpu\n"
     ]
    }
   ],
   "source": [
    "for name, param in model2.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model2.cpu()\n",
    "surgeon2.extended_embedding.cpu()\n",
    "surgeon2.extended_unembedding.cpu()\n",
    "surgeon2.backup_embed_tokens.cpu()\n",
    "surgeon2.backup_lm_head.cpu()\n",
    "surgeon.extended_embedding.cpu()\n",
    "surgeon.extended_unembedding.cpu()\n",
    "surgeon.backup_embed_tokens.cpu()\n",
    "surgeon.backup_lm_head.cpu()\n",
    "gc.collect()\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    torch.cuda.set_device(i) \n",
    "    torch.cuda.empty_cache() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
