# @package _global_

defaults:
  - train/ppo/no_pause_peft
  - override /rl_algorithm: pretraining_value_head
  - override /rl_algorithm/policy/model/value_head: torch_transformer # mlp or torch_transformer

name: "pretraining value head"
run_name: "pretraining_value_head"

trainer:
  n_steps_before_validation: 10
  n_outer_loops: 10
  
rl_algorithm:
  n_steps: 1
  n_envs: 32 #  is the number of times rollout() is called in each outer loop, 
  batch_size: 4 # number of rollouts to be used in each training step
  loss_computed_in_forward_pass: true
  ent_coef: 0

  policy:
    model:
      language_model:
        pretrained_model_name_or_path: /dlabscratch1/baldwin/pause2/PauseToken/logs/sft/runs/2024-10-21_10-08-12/final

    # path to where the model should be saved, if null, it will be saved in /data/value_head
  saving_path: "./data/value_heads/${clean_path:${rl_algorithm.policy.model.language_model.pretrained_model_name_or_path}}_transformer.pth"