# @package _global_

defaults:
  - override /data: gsm8k
  - override /rl_algorithm: star_on_policy
  - override /rl_algorithm/policy/model/language_model: pause_from_pretrained
  - override /rl_algorithm/policy/model/peft_config: null #peft is already there
  - override /rl_algorithm/reward: gsm8k
  - override /metrics: gsm8k

name: "star on gsm8k"
run_name: "debug"
task_name: "train"



data:
  additional_transformation:
    _target_: functools.partial
    _args_: 
      - ${get_method:src.utils.trainer_utils.inference_formatting_function}
    eos_token: ${get_obj_attr:${rl_algorithm.policy.model.tokenizer},[eos_token]}
  

trainer:
  inner_loop_timesteps: 2 # this gets translated into the number of times rollouts are done for each outer loop
  n_outer_loops: 3 # number of times an outer loop is done, then validation is done and the model is saved.
  progress_bar: true
  num_val_samples: 10
  save_top_k: 3
  metric_for_best_model: "val/accuracy"
  #whether the metric for the best model is min or max (True = min (the lower the better), False = max (the higher the better))
  metric_for_best_model_mode_is_min: false



rl_algorithm:
  n_steps: 5 # used only in collect rollouts. the size of rollouts is n_envs * n_steps
  n_envs: 4 # inner_loop_timesteps * n_envs is the number of times rollout() is called in each outer loop, 
  batch_size: 2 # number of rollouts to be used in each training step
  loss_computed_in_forward_pass: true
  buffer:
    advantage_threshold: 0

  policy:
    model:
      language_model:
        pretrained_model_name_or_path: /dlabscratch1/baldwin/pause2/PauseToken/logs/sft/runs/2024-08-29_10-23-33/final
        post_instanciation_method_calls:
          - method: unfreeze_all
      