# @package _global_
defaults:
  - train/sft/default
  - override /rl_algorithm/policy/model/peft_config: null


run_name: "Llama3B_sft"

rl_algorithm:
  n_steps: 9 # used only in collect rollouts. the size of rollouts is n_envs * n_steps which should equal dataset size
  n_envs: 16 #  is the number of times rollout() is called in each outer loop, 
  batch_size: 4 # number of rollouts to be used in each training step
  n_grad_accumulation_steps: 1
  data_collator:
    add_context_to_response: true

  policy:
    model:
      language_model:
        pretrained_model_name_or_path: /dlabscratch1/public/llm_weights/meta-llama_llama-3.2-3b
        torch_dtype: 
          _target_: src.utils.hydra_custom_resolvers.get_module_attr
          module_and_attr: torch.bfloat16
    generation:
      train:
        generation_config:
          pad_token_id: 128002

      test:
        generation_config:
          pad_token_id: 128002