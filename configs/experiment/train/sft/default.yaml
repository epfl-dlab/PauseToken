# @package _global_
defaults:
  - override /data: gsm8k
  - override /rl_algorithm: reinforce
  - override /rl_algorithm/policy: llm_base_policy
  - override /rl_algorithm/policy/model/language_model:  auto_model_for_causal_lm
  - override /rl_algorithm/policy/model/peft_config: ???
  - override /rl_algorithm/reward: gsm8k
  - override /metrics: gsm8k

name: "SFT"
task_name: "train"
run_name: ???

data:
  additional_transformation:
    _target_: functools.partial
    _args_: 
      - ${get_method:src.utils.trainer_utils.inference_formatting_function}
    eos_token: ${get_obj_attr:${rl_algorithm.policy.model.tokenizer},[eos_token]}
  

trainer:
  trainer_callbacks:
    init_portion: 1.0
    final_portion: 1.0
  n_steps_before_validation: 6336
  n_outer_loops: 10
  progress_bar: true
  # learn_callbacks:
  #   _target_: lm_stable_baselines.callbacks.lm_callback.StarProgressBarCallback
  num_val_samples: 748
  save_top_k: 3
  metric_for_best_model: "val/accuracy"
  #whether the metric for the best model is min or max (True = min (the lower the better), False = max (the higher the better))
  metric_for_best_model_mode_is_min: false
  disable_peft_first_inference: false

rl_algorithm:
  learning_rate: 1e-5
  n_steps: 9 # used only in collect rollouts. the size of rollouts is n_envs * n_steps which should equal dataset size
  n_envs: 32 #  is the number of times rollout() is called in each outer loop, 
  batch_size: 4 # number of rollouts to be used in each training step
  n_grad_accumulation_steps: 1
  loss_computed_in_forward_pass: true
  use_base_model_for_learning: false
  ent_coef: 0
  buffer:
    advantage_threshold: 0
    # model_dtype: torch.bfloat16   # Only if you want to use bfloat16 in model.

  policy:
    model:
      language_model:
        pretrained_model_name_or_path: ???
      value_head: null
    per_token_log_prob: true
        
  reward:
    inverse_neg_log_likelihood: true
    correctness_reward_weight: 1.0
  
test: true
test_batch_size: 8
test_formatting_func: null
