# @package _global_

defaults:
  - train/online_star_exp/no_pause_peft
  - override /rl_algorithm/policy/model/peft_config: null

name: "online star on gsm8k"
task_name: "train"


run_name: "star_no_pause_tinyllama"


trainer:
  disable_peft_first_inference: false
  

rl_algorithm:
  # used only in collect rollouts. the size of rollouts is n_envs * n_steps which should equal dataset size
  # for gsm8k it is 396 * 99 = 1584
  n_steps: 9 
  policy:
    model:
      language_model:
        pretrained_model_name_or_path: /dlabscratch1/baldwin/pause2/PauseToken/logs/sft/runs/2024-12-16_14-35-04/final

