_target_: src.model.components.value_head.torch_transformer.TransformerValueHead

hidden_dim: 4096
transformer_config:
  _target_: transformers.GPT2Model
  config:
    _target_: transformers.GPT2Config
    vocab_size: 1
    n_embd: ${...hidden_dim}
    n_layer: 8
    n_head: 8
    n_positions: 1025


  # _target_: torch.nn.TransformerEncoder
  # num_layers: 6
  # encoder_layer:
  #   _target_: torch.nn.TransformerEncoderLayer
  #   d_model: 
  #   nhead: 8
  #   batch_first: true
  

# transformer_config:
#   _target_: transformers.BertModel
#   config:
#     _target_: transformers.BertConfig
#     vocab_size: 1
#     hidden_size: ${...hidden_dim}
#     num_hidden_layers: 4
    
#     num_attention_heads: 8
#     intermediate_size: 4096
#     max_position_embeddings: 1025
#     position_embedding_type: 'relative_key_query'