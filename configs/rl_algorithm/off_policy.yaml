defaults:
  - environment: language_model_env
  - reward: default
  - policy: llm_base_policy
  - buffer: lm_replay_buffer



_target_: stable_baselines3.common.off_policy_algorithm.OffPolicyAlgorithm
n_envs: 1
## TODO: What to do here ? Should I just put this in another config file ?
policy_class: ${.policy._target_}
policy_kwargs: ${get_dict_except:${.policy},"_target_"}

# #fetch _target_ argument from buffer
replay_buffer_class: ${.buffer._target_}
# #fetch all arguments from buffer except _target_
replay_buffer_kwargs: ${get_dict_except:${.buffer},"_target_"}

learning_rate: 1e-5
buffer_size: 1000000
learning_starts: 0
batch_size: 256
tau: 0.005
gamma: 0.99
train_freq: 10
gradient_steps: 1
action_noise: null
optimize_memory_usage: false
stats_window_size: 100
tensorboard_log: null
verbose: 0
device: "auto"
support_multi_env: false
monitor_wrapper: true
seed: null
use_sde: false
sde_sample_freq: -1
use_sde_at_warmup: false
sde_support: true
supported_action_spaces: null



