python src/train.py experiment=train/ppo/llama1B/curr_sft_1_ep_beta rl_algorithm.policy.ft_on_action_only=false \
rl_algorithm.ent_coef=0.009 rl_algorithm.vf_coef=0.01 run_name=llama1B_ppo_beta_truerl2unif \
trainer.callbacks.portion_annealers.init_alpha=0.1 trainer.callbacks.portion_annealers.final_alpha=1.0 \
trainer.callbacks.portion_annealers.init_beta=50.0 trainer.callbacks.portion_annealers.final_beta=1.0 \
trainer.callbacks.portion_annealers.warmup_timesteps=0 trainer.callbacks.portion_annealers.total_timesteps=5

python src/train.py experiment=train/ppo/llama1B/curr_sft_1_ep_beta rl_algorithm.policy.ft_on_action_only=true \
rl_algorithm.ent_coef=0.009 rl_algorithm.vf_coef=0.01 run_name=llama1B_ppo_beta_ftact_truerl2unif \
trainer.callbacks.portion_annealers.init_alpha=0.1 trainer.callbacks.portion_annealers.final_alpha=1.0 \
trainer.callbacks.portion_annealers.init_beta=50.0 trainer.callbacks.portion_annealers.final_beta=1.0 \
trainer.callbacks.portion_annealers.warmup_timesteps=0 trainer.callbacks.portion_annealers.total_timesteps=5